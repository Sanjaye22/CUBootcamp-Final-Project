{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"C:\\Program Files\\Java\\jdk1.8.0_271\"\n",
    "os.environ[\"HADOOP_HOME\"] = \"C:\\Installations\\Hadoop\"\n",
    "os.environ[\"SPARK_HOME\"] = \"D:\\spark-2.4.5-bin-hadoop2.7\\spark-2.4.5-bin-hadoop2.7\"\n",
    "\n",
    "# Start a SparkSession\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "from pyspark import SparkFiles\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import *\n",
    "from pyspark.ml.classification import *\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "from pyspark.ml.regression import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create builder for SparkSession\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.memory\", \"10g\") \\\n",
    "    .appName(\"CloudETLProject\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read csv\n",
    "import pandas as pd\n",
    "pd_test = pd.read_csv('../cleaned_nlp_data/testfinal.csv', sep=',')\n",
    "pd_train = pd.read_csv('../cleaned_nlp_data/trainfinal.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a PySpark schema\n",
    "mySchema = StructType([ StructField(\"uniqueID\", StringType(), True)\\\n",
    "                       ,StructField(\"drugName\", StringType(), True)\\\n",
    "                       ,StructField(\"condition\", StringType(), True)\\\n",
    "                       ,StructField(\"review\", StringType(), True)\\\n",
    "                       ,StructField(\"rating\", IntegerType(), True)\\\n",
    "                       ,StructField(\"date\", StringType(), True)\\\n",
    "                       ,StructField(\"usefulCount\", StringType(), True)\\\n",
    "                     ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create PySpark dataframes\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark import SparkConf, SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "df_test = sqlContext.createDataFrame(pd_test, schema=mySchema)\n",
    "df_train = sqlContext.createDataFrame(pd_train, schema=mySchema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns that won't be used\n",
    "drop_df = df_test.drop('uniqueID','drugName','condition', 'date', 'usefulCount' ).collect()\n",
    "drop_df_two = df_train.drop('uniqueID','drugName','condition', 'date', 'usefulCount' ).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+--------------------+------+\n|              review|rating|\n+--------------------+------+\n|gave me rapid hea...|     0|\n|    it cured my mrsa|     1|\n|i have been on zy...|     1|\n|it didnt work as ...|     1|\n|i have had  major...|     1|\n|i had mrsa inf la...|     1|\n|i got a mrsa stap...|     1|\n|very satisfied wi...|     1|\n|effectiveness las...|     0|\n|my psa was going ...|     1|\n|on zytiga for  mo...|     1|\n|began zytiga with...|     1|\n|had tried clariti...|     1|\n|this medicine wor...|     1|\n|i have had cholin...|     1|\n|after travelling ...|     1|\n|i suffered from m...|     1|\n|i recently had te...|     1|\n|it works great fo...|     1|\n|had hives nearly ...|     1|\n+--------------------+------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Show test dataframe with dropped columns\n",
    "test_df = spark.createDataFrame(drop_df)\n",
    "test_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+--------------------+------+\n|              review|rating|\n+--------------------+------+\n|it has no side ef...|     1|\n|my son is halfway...|     1|\n|i used to take an...|     0|\n|this is my first ...|     1|\n|suboxone has comp...|     1|\n|nd day on mg star...|     0|\n|he pulled out but...|     0|\n|abilify changed m...|     1|\n| i ve had  nothin...|     0|\n|i had been on the...|     1|\n|i have been on th...|     1|\n|i have taken anti...|     1|\n|i had crohns with...|     0|\n|have a little bit...|     0|\n|started nexplanon...|     0|\n|i have been takin...|     1|\n|this drug worked ...|     1|\n|ive been taking a...|     1|\n|ive been on every...|     1|\n|i have been on ta...|     1|\n+--------------------+------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Show train dataframe with dropped columns\n",
    "train_df = spark.createDataFrame(drop_df_two)\n",
    "train_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pipeline\n",
    "def build_trigrams(inputCol=[\"review\",\"rating\"], n=3):\n",
    "\n",
    "# Tokenizer converts input text into a stream of tokens, where each token is a separate word\n",
    "    tokenizer = [Tokenizer(inputCol=\"review\", outputCol=\"words\")]\n",
    "\n",
    "# Get rid of stop words\n",
    "    stopremove = [StopWordsRemover(inputCol='words',outputCol='stop_tokens')]\n",
    "\n",
    "# Stem the words\n",
    "\n",
    "# Creates a column for every word, two and three words (n=3). Looks for sequences of words from a given sample of text or speech. \n",
    "    ngrams = [\n",
    "        NGram(n=i, inputCol=\"stop_tokens\", outputCol=\"{0}_grams\".format(i))\n",
    "        for i in range(1, n + 1)\n",
    "    ]\n",
    "\n",
    "# Count vectorizer converts a collection of text (i.e review rows) to a vector term or token counts. Counts the number of times a word repeats. \n",
    "    cv = [\n",
    "        CountVectorizer(vocabSize=2**15,inputCol=\"{0}_grams\".format(i),\n",
    "            outputCol=\"{0}_tf\".format(i))\n",
    "        for i in range(1, n + 1)\n",
    "    ]\n",
    "\n",
    "# Inverse document frequency scales down the term weights of terms with high collection frequency. If a word appears very often, the weight of that word is decreased. (i.e the words drug/medication appears often because this is a dataset about those things)\n",
    "    idf = [IDF(inputCol=\"{0}_tf\".format(i), outputCol=\"{0}_tfidf\".format(i), minDocFreq=5) for i in range(1, n + 1)]\n",
    "\n",
    "# Vector assembler combines raw features and features into a single feature vector. We are using it to combine cv and idf. \n",
    "    assembler = [VectorAssembler(\n",
    "        inputCols=[\"{0}_tfidf\".format(i) for i in range(1, n + 1)],\n",
    "        outputCol=\"features\"\n",
    "    )]\n",
    "# String indexer converts text to numeric. \n",
    "    label_stringIdx = [StringIndexer(inputCol = \"rating\", outputCol = \"label\")]\n",
    "\n",
    "# Logistic regression is the model being used. Using logistic regression because we are taking in two values (binary). For our purposes it will be 0 and 1 (negative/positive). \n",
    "    rf = [RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", numTrees=1)]\n",
    "    return Pipeline(stages=tokenizer + stopremove + ngrams + cv + idf + assembler + label_stringIdx + rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the model \n",
    "trigram_pipelineFit = build_trigrams().fit(train_df)\n",
    "test_results = trigram_pipelineFit.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy Score: 0.7157\nROC-AUC: 0.5332\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "evaluator = RegressionEvaluator()\n",
    "\n",
    "# Accuracy might not always be the right measure, especially if target class is not balanced. \n",
    "accuracy = test_results.filter(test_results.label == test_results.prediction).count() / float(test_results.count())\n",
    "# Apply a confusion matrix for TP, TN and FN. \n",
    "roc_auc = evaluator.evaluate(test_results)\n",
    "print(\"Accuracy Score: {0:.4f}\".format(accuracy))\n",
    "print(\"ROC-AUC: {0:.4f}\".format(roc_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "y_true = test_results.select(['label']).collect()\n",
    "y_pred = test_results.select(['prediction']).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "              precision    recall  f1-score   support\n\n         0.0       0.72      0.96      0.82     37559\n         1.0       0.61      0.15      0.24     16207\n\n    accuracy                           0.72     53766\n   macro avg       0.67      0.56      0.53     53766\nweighted avg       0.69      0.72      0.65     53766\n\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[35998  1561]\n [13727  2480]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+------+------+\n|rating| count|\n+------+------+\n|     1|113209|\n|     0| 48088|\n+------+------+\n\n"
     ]
    }
   ],
   "source": [
    "train_df.groupBy(\"rating\").count().orderBy(desc(\"count\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "# trigram_pipelineFit.save(\"random_forest_eval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}